{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CNN_on_CIFAR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "id": "ut8ZtSPbS-2c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN on CIFAR\n",
        "#### By MMA"
      ]
    },
    {
      "metadata": {
        "id": "YrIfceQAq6wE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Define Cifar API as MNIST API**"
      ]
    },
    {
      "metadata": {
        "id": "7KVLIFirUO1_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DataSet:\n",
        "    def __init__(self, data, labels, one_hot=False):\n",
        "        self.images = np.array(data, ndmin=2)\n",
        "        if one_hot:\n",
        "            self.labels = self.__one_hot(labels)\n",
        "        else:\n",
        "            self.labels = labels\n",
        "\n",
        "        self.num_examples = len(data)\n",
        "        self.__index = 0\n",
        "\n",
        "    @staticmethod\n",
        "    def __one_hot(labels):\n",
        "        new_label = []\n",
        "        for label in labels:\n",
        "            row = []\n",
        "            for i in range(10):\n",
        "                if i == label:\n",
        "                    row.append(1)\n",
        "                else:\n",
        "                    row.append(0)\n",
        "            new_label.append(row)\n",
        "        return np.array(new_label, ndmin=2)\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        if self.__index + batch_size > self.images.shape[0]:\n",
        "            self.__index = 0\n",
        "        x, y = self.images[self.__index: self.__index + batch_size], self.labels[self.__index: self.__index + batch_size]\n",
        "        self.__index = (self.__index + batch_size) % self.images.shape[0]\n",
        "        return x, y\n",
        "\n",
        "    def normalize(self):\n",
        "        self.images = np.array(list(map(lambda x: x/255, self.images)))\n",
        "\n",
        "\n",
        "class Cifar:\n",
        "    def __init__(self, batches, test, one_hot=False):\n",
        "        data = []\n",
        "        labels = []\n",
        "        for batch in batches:\n",
        "            for i in range(len(batch[b'data'])):\n",
        "                data.append(batch[b'data'][i])\n",
        "                labels.append(batch[b'labels'][i])\n",
        "\n",
        "        self.train = DataSet(data[:int(4 * len(data) / 5)], labels[:int(4 * len(data) / 5)], one_hot)\n",
        "        self.validation = DataSet(data[int(4 * len(data) / 5):], labels[int(4 * len(data) / 5):], one_hot)\n",
        "        self.test = DataSet(test[b'data'], test[b'labels'], one_hot)\n",
        "\n",
        "    def normalize_data(self):\n",
        "        self.train.normalize()\n",
        "        self.validation.normalize()\n",
        "        self.test.normalize()\n",
        "\n",
        "\n",
        "def read_data_sets(url, one_hot=False):\n",
        "    batches = []\n",
        "    for i in range(1, 6):\n",
        "        with open(url + 'data_batch_' + str(i), 'rb') as f:\n",
        "            batches.append(pickle.load(f, encoding='bytes'))\n",
        "            f.close()\n",
        "    with open(url + 'test_batch', 'rb') as f:\n",
        "        test = pickle.load(f, encoding='bytes')\n",
        "        f.close()\n",
        "    return Cifar(batches, test, one_hot)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ra3wx1YYS-2h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "blvc7Ko3S-2u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Training Parameter"
      ]
    },
    {
      "metadata": {
        "id": "BcYsANJGS-2x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 50000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q67FxRJES-26",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Read Data"
      ]
    },
    {
      "metadata": {
        "id": "R-F5PXlFS-2_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cifar = read_data_sets(\"cifar-10-batches-py/\", one_hot=True)\n",
        "cifar.normalize_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_c41uqXsS-3F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Network Parameter"
      ]
    },
    {
      "metadata": {
        "id": "r_n9WRNIS-3I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_input = 3072\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lDOBufCHS-3Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Tensorflow Graph Input"
      ]
    },
    {
      "metadata": {
        "id": "D2r2Pua9S-3U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(dtype=tf.float32, shape=(None, num_input), name=\"Inputs\")\n",
        "Y = tf.placeholder(dtype=tf.float32, shape=(None, num_classes), name=\"Lables\")\n",
        "keep_prob = tf.placeholder(dtype=tf.float32) # DropOut ( Keep Probability )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rA5Vq1dhS-3h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Create some wrappers"
      ]
    },
    {
      "metadata": {
        "id": "AmguP6FaS-3k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv2d(x, W, b, strides=1):\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0vOqqa12S-3t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def maxpool2d(x, k=2, stride=1):\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, stride, stride, 1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5LetHIbTS-32",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Create Model"
      ]
    },
    {
      "metadata": {
        "id": "RXwL1Ai3S-38",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "conv1 = None\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "    global conv1\n",
        "    x = tf.reshape(x, shape=[-1, 32, 32, 3])\n",
        "    \n",
        "    # Convolution Layer 1\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max Pooling\n",
        "    conv1 = maxpool2d(conv1, k=3, stride=1)\n",
        "    \n",
        "    # Convolution Layer 2\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max Pooling\n",
        "    conv2 = maxpool2d(conv2, k=3, stride=1)\n",
        "    \n",
        "    # Fully Connected Layer\n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "    \n",
        "    # Output\n",
        "    output = tf.add(tf.matmul(fc1, weights['output']), biases['output'])\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fz2nyqpLS-4D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Store Layer Weights and Biases"
      ]
    },
    {
      "metadata": {
        "id": "G7xNCpysS-4G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights = {\n",
        "    'wc1': tf.Variable(tf.truncated_normal([5, 5, 3, 64], 0, 0.01)),\n",
        "    'wc2': tf.Variable(tf.truncated_normal([5, 5, 64, 64], 0, 0.01)),\n",
        "    'wd1': tf.Variable(tf.truncated_normal([32*32*64, 512], 0, 0.01)),\n",
        "    'output': tf.Variable(tf.truncated_normal([512, num_classes], 0, 0.01))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.zeros([64])),\n",
        "    'bc2': tf.Variable(tf.zeros([64])),\n",
        "    'bd1': tf.Variable(tf.zeros([512])),\n",
        "    'output': tf.Variable(tf.zeros([num_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R89ku_htS-4L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Construct Model"
      ]
    },
    {
      "metadata": {
        "id": "sRKCI0-vS-4Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = conv_net(X, weights, biases, keep_prob)\n",
        "prediction = tf.nn.softmax(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KZ3zaMLPS-4d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Define loss and optimizer"
      ]
    },
    {
      "metadata": {
        "id": "psGXNUVDS-4h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "regularizers = tf.nn.l2_loss(weights['wc1']) + tf.nn.l2_loss(weights['wc2']) + \\\n",
        "               tf.nn.l2_loss(weights['wd1']) + tf.nn.l2_loss(weights['output'])\n",
        "loss = tf.reduce_mean(cross_entropy + 0.01 * regularizers)\n",
        "optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
        "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32), name=\"accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ozpDGconqTGk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Define Sumarries**"
      ]
    },
    {
      "metadata": {
        "id": "vaxaB_7NqYJp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_train_summary_op = tf.summary.scalar(\"Loss_train\", loss)\n",
        "acc_train_summary_op = tf.summary.scalar(\"accuracy_train\", accuracy_op)\n",
        "\n",
        "acc_val_summary_op = tf.summary.scalar(\"accuracy_validation\", accuracy_op)\n",
        "loss_val_summary_op = tf.summary.scalar(\"Loss_validation\", loss)\n",
        "\n",
        "filewriter = tf.summary.FileWriter(\"./graphs_cnn\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bSxb7mnCS-4m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Start Training"
      ]
    },
    {
      "metadata": {
        "id": "dsLsTnZpS-4o",
        "colab_type": "code",
        "outputId": "2b50e03d-7319-46ac-d5aa-66d0a9521090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4359
        }
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess_cnn = tf.Session()\n",
        "filewriter.add_graph(sess_cnn.graph)\n",
        "sess_cnn.run(init)\n",
        "for i in range(EPOCHS):\n",
        "    batch_xs, batch_ys = cifar.train.next_batch(BATCH_SIZE)\n",
        "    sess_cnn.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.5})\n",
        "    if i % 200 == 0:\n",
        "        print(i)\n",
        "#           acc = sess_cnn.run(accuracy_op, feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 1.0})\n",
        "#           print(\"round : \", i , \" Acc : \", acc)\n",
        "        b1, b2 = (sess_cnn.run([loss_train_summary_op, acc_train_summary_op],\n",
        "                               feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 1.0}))\n",
        "        filewriter.add_summary(b1, i)\n",
        "        filewriter.add_summary(b2, i)\n",
        "\n",
        "        batch_xs_v, batch_ys_v = cifar.validation.next_batch(BATCH_SIZE)\n",
        "        b1, b2 = (sess_cnn.run([acc_val_summary_op, loss_val_summary_op],\n",
        "                               feed_dict={X: batch_xs_v, Y: batch_ys_v, keep_prob: 1.0}))\n",
        "        filewriter.add_summary(b1, i)\n",
        "        filewriter.add_summary(b2, i)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "200\n",
            "400\n",
            "600\n",
            "800\n",
            "1000\n",
            "1200\n",
            "1400\n",
            "1600\n",
            "1800\n",
            "2000\n",
            "2200\n",
            "2400\n",
            "2600\n",
            "2800\n",
            "3000\n",
            "3200\n",
            "3400\n",
            "3600\n",
            "3800\n",
            "4000\n",
            "4200\n",
            "4400\n",
            "4600\n",
            "4800\n",
            "5000\n",
            "5200\n",
            "5400\n",
            "5600\n",
            "5800\n",
            "6000\n",
            "6200\n",
            "6400\n",
            "6600\n",
            "6800\n",
            "7000\n",
            "7200\n",
            "7400\n",
            "7600\n",
            "7800\n",
            "8000\n",
            "8200\n",
            "8400\n",
            "8600\n",
            "8800\n",
            "9000\n",
            "9200\n",
            "9400\n",
            "9600\n",
            "9800\n",
            "10000\n",
            "10200\n",
            "10400\n",
            "10600\n",
            "10800\n",
            "11000\n",
            "11200\n",
            "11400\n",
            "11600\n",
            "11800\n",
            "12000\n",
            "12200\n",
            "12400\n",
            "12600\n",
            "12800\n",
            "13000\n",
            "13200\n",
            "13400\n",
            "13600\n",
            "13800\n",
            "14000\n",
            "14200\n",
            "14400\n",
            "14600\n",
            "14800\n",
            "15000\n",
            "15200\n",
            "15400\n",
            "15600\n",
            "15800\n",
            "16000\n",
            "16200\n",
            "16400\n",
            "16600\n",
            "16800\n",
            "17000\n",
            "17200\n",
            "17400\n",
            "17600\n",
            "17800\n",
            "18000\n",
            "18200\n",
            "18400\n",
            "18600\n",
            "18800\n",
            "19000\n",
            "19200\n",
            "19400\n",
            "19600\n",
            "19800\n",
            "20000\n",
            "20200\n",
            "20400\n",
            "20600\n",
            "20800\n",
            "21000\n",
            "21200\n",
            "21400\n",
            "21600\n",
            "21800\n",
            "22000\n",
            "22200\n",
            "22400\n",
            "22600\n",
            "22800\n",
            "23000\n",
            "23200\n",
            "23400\n",
            "23600\n",
            "23800\n",
            "24000\n",
            "24200\n",
            "24400\n",
            "24600\n",
            "24800\n",
            "25000\n",
            "25200\n",
            "25400\n",
            "25600\n",
            "25800\n",
            "26000\n",
            "26200\n",
            "26400\n",
            "26600\n",
            "26800\n",
            "27000\n",
            "27200\n",
            "27400\n",
            "27600\n",
            "27800\n",
            "28000\n",
            "28200\n",
            "28400\n",
            "28600\n",
            "28800\n",
            "29000\n",
            "29200\n",
            "29400\n",
            "29600\n",
            "29800\n",
            "30000\n",
            "30200\n",
            "30400\n",
            "30600\n",
            "30800\n",
            "31000\n",
            "31200\n",
            "31400\n",
            "31600\n",
            "31800\n",
            "32000\n",
            "32200\n",
            "32400\n",
            "32600\n",
            "32800\n",
            "33000\n",
            "33200\n",
            "33400\n",
            "33600\n",
            "33800\n",
            "34000\n",
            "34200\n",
            "34400\n",
            "34600\n",
            "34800\n",
            "35000\n",
            "35200\n",
            "35400\n",
            "35600\n",
            "35800\n",
            "36000\n",
            "36200\n",
            "36400\n",
            "36600\n",
            "36800\n",
            "37000\n",
            "37200\n",
            "37400\n",
            "37600\n",
            "37800\n",
            "38000\n",
            "38200\n",
            "38400\n",
            "38600\n",
            "38800\n",
            "39000\n",
            "39200\n",
            "39400\n",
            "39600\n",
            "39800\n",
            "40000\n",
            "40200\n",
            "40400\n",
            "40600\n",
            "40800\n",
            "41000\n",
            "41200\n",
            "41400\n",
            "41600\n",
            "41800\n",
            "42000\n",
            "42200\n",
            "42400\n",
            "42600\n",
            "42800\n",
            "43000\n",
            "43200\n",
            "43400\n",
            "43600\n",
            "43800\n",
            "44000\n",
            "44200\n",
            "44400\n",
            "44600\n",
            "44800\n",
            "45000\n",
            "45200\n",
            "45400\n",
            "45600\n",
            "45800\n",
            "46000\n",
            "46200\n",
            "46400\n",
            "46600\n",
            "46800\n",
            "47000\n",
            "47200\n",
            "47400\n",
            "47600\n",
            "47800\n",
            "48000\n",
            "48200\n",
            "48400\n",
            "48600\n",
            "48800\n",
            "49000\n",
            "49200\n",
            "49400\n",
            "49600\n",
            "49800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sU_PV9s4S-4x",
        "colab_type": "code",
        "outputId": "e336a806-d41b-4bb6-b7ed-f00c6c0c2785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "[accuracy] = sess_cnn.run([accuracy_op], feed_dict={X: cifar.test.images[:256],\n",
        "                                                    Y: cifar.test.labels[:256],\n",
        "                                                    keep_prob: 1.0})\n",
        "print(\"The accuracy of test:\" + str(accuracy))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of test:0.55078125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jkW32ISBt4_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}